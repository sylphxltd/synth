# Streaming Support Analysis
## Synth vs Unified - Streaming èƒ½åŠ›å¯¹æ¯”

---

## ğŸ“Š ä»€ä¹ˆæ˜¯ Streaming Parsing?

### **DOM-style (ä¼ ç»Ÿæ–¹å¼)**
```javascript
// ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ–‡æ¡£åˆ°å†…å­˜
const ast = parse(entireDocument)
// æ„å»ºå®Œæ•´çš„ AST æ ‘
// ç„¶åå¤„ç†
```

**ä¼˜ç‚¹**: å¯ä»¥éšæœºè®¿é—®ä»»ä½•èŠ‚ç‚¹
**ç¼ºç‚¹**: å¤§æ–‡ä»¶å ç”¨å¤§é‡å†…å­˜

---

### **SAX-style (Streaming æ–¹å¼)**
```javascript
// è¾¹è¯»è¾¹å¤„ç†ï¼Œä¸ä¿ç•™å®Œæ•´æ ‘
parser.on('startElement', (name) => { /* å¤„ç† */ })
parser.on('text', (text) => { /* å¤„ç† */ })
parser.on('endElement', (name) => { /* å¤„ç† */ })
```

**ä¼˜ç‚¹**: å†…å­˜å ç”¨å›ºå®šï¼Œå¯å¤„ç†ä»»æ„å¤§æ–‡ä»¶
**ç¼ºç‚¹**: åªèƒ½é¡ºåºè®¿é—®ï¼Œéš¾ä»¥å®ç°å¤æ‚è½¬æ¢

---

## ğŸ” Unified çš„ Streaming æ”¯æŒ

### **è°ƒç ”ç»“æœ: âŒ Unified ä¸æ”¯æŒ streaming**

**åŸå› :**
1. **DOM-based æ¶æ„**
   - unified/remark/rehype éƒ½æ˜¯æ„å»ºå®Œæ•´ AST
   - å¿…é¡»å…ˆ parse æ•´ä¸ªæ–‡æ¡£
   - æ‰èƒ½è¿›è¡Œ transform

2. **æ’ä»¶ç³»ç»Ÿä¾èµ–å®Œæ•´æ ‘**
   ```javascript
   // unified æ’ä»¶éœ€è¦è®¿é—®æ•´ä¸ªæ ‘
   function myPlugin() {
     return (tree) => {
       // tree å¿…é¡»æ˜¯å®Œæ•´çš„ AST
       visit(tree, 'heading', (node) => {
         // éœ€è¦è®¿é—®çˆ¶èŠ‚ç‚¹ã€å…„å¼ŸèŠ‚ç‚¹ç­‰
       })
     }
   }
   ```

3. **å†…å­˜å ç”¨**
   - å¤§æ–‡ä»¶ (10MB+) ä¼šå ç”¨å¤§é‡å†…å­˜
   - Parse + Transform + Compile éƒ½åœ¨å†…å­˜ä¸­
   - å¯èƒ½å¯¼è‡´ OOM (Out of Memory)

---

## ğŸ¯ Synth å½“å‰çš„ Streaming æ”¯æŒ

### **çŠ¶æ€: âŒ æš‚ä¸æ”¯æŒçœŸæ­£çš„ streaming**

**åŸå› :**
1. **Arena-based æ¶æ„**
   - æˆ‘ä»¬çš„è®¾è®¡ä¹Ÿæ˜¯ DOM-style
   - èŠ‚ç‚¹å­˜å‚¨åœ¨è¿ç»­æ•°ç»„ä¸­
   - éœ€è¦å®Œæ•´è§£ææ‰èƒ½æ„å»ºç´¢å¼•

2. **Query Index ä¾èµ–å®Œæ•´æ ‘**
   - ç´¢å¼•æ„å»ºéœ€è¦éå†æ‰€æœ‰èŠ‚ç‚¹
   - æ— æ³•å¢é‡æ„å»º

**ä½†æ˜¯...**

---

## âœ¨ Synth çš„ä¼˜åŠ¿ï¼šæˆ‘ä»¬å¯ä»¥è½»æ¾æ”¯æŒ Streaming!

### **ä¸ºä»€ä¹ˆæˆ‘ä»¬æ¯” unified æ›´é€‚åˆåš streaming?**

#### 1. **Arena Allocator å¤©ç„¶æ”¯æŒ Streaming**
```typescript
// å¯ä»¥è¾¹è¯»è¾¹æ·»åŠ èŠ‚ç‚¹
const tree = createTree('markdown', '')

// æµå¼æ·»åŠ èŠ‚ç‚¹
stream.on('data', (chunk) => {
  const nodes = parseChunk(chunk)
  for (const node of nodes) {
    addNode(tree, node) // O(1) è¿½åŠ 
  }
})
```

#### 2. **Flat Array = å¤©ç„¶çš„ Append Buffer**
```typescript
// æˆ‘ä»¬çš„èŠ‚ç‚¹å­˜å‚¨
nodes: [node0, node1, node2, ...]
       ğŸ‘† å¯ä»¥æŒç»­è¿½åŠ ï¼Œæ— éœ€é‡æ–°åˆ†é…
```

#### 3. **Node Pool æ”¯æŒèŠ‚ç‚¹é‡ç”¨**
```typescript
// Streaming æ—¶å¯ä»¥å¤ç”¨å·²å¤„ç†çš„èŠ‚ç‚¹
stream.on('chunk-processed', (nodes) => {
  globalNodePool.releaseMany(nodes)
})
```

#### 4. **Batch Processing å¤©ç„¶æ”¯æŒæµå¼å¤„ç†**
```typescript
// æ¯ä¸ª chunk ä½œä¸ºä¸€ä¸ª batch
batchProcess(tree, chunkNodeIds, visitor)
```

---

## ğŸš€ Synth Streaming å®ç°æ–¹æ¡ˆ

### **æ–¹æ¡ˆ 1: SAX-style Event Streaming** (æ¨è)

```typescript
export class StreamingParser {
  private tree: Tree
  private eventEmitter: EventEmitter

  // SAX-style events
  on(event: 'node', callback: (node: BaseNode) => void): void
  on(event: 'end', callback: (tree: Tree) => void): void

  // æµå¼è§£æ
  parseStream(stream: ReadableStream): void {
    stream.on('data', (chunk) => {
      const nodes = this.parseChunk(chunk)

      for (const node of nodes) {
        // æ·»åŠ åˆ°æ ‘
        addNode(this.tree, node)

        // è§¦å‘äº‹ä»¶
        this.emit('node', node)
      }
    })

    stream.on('end', () => {
      this.emit('end', this.tree)
    })
  }
}
```

**ä½¿ç”¨ç¤ºä¾‹:**
```typescript
const parser = new StreamingParser()

let headingCount = 0

parser.on('node', (node) => {
  if (node.type === 'heading') {
    headingCount++
    console.log('Found heading:', node)
  }
})

parser.on('end', (tree) => {
  console.log('Total headings:', headingCount)
  console.log('Final tree:', tree)
})

// å¤„ç†å¤§æ–‡ä»¶
const stream = fs.createReadStream('huge-file.md')
parser.parseStream(stream)
```

**ä¼˜åŠ¿:**
- âœ… å†…å­˜å ç”¨å›ºå®š
- âœ… å¯å¤„ç†ä»»æ„å¤§æ–‡ä»¶
- âœ… å®æ—¶å¤„ç†ï¼ˆè¾¹è¯»è¾¹å¤„ç†ï¼‰
- âœ… æœ€åä»å¯å¾—åˆ°å®Œæ•´ AST

---

### **æ–¹æ¡ˆ 2: Chunked Processing**

```typescript
export class ChunkedParser {
  // åˆ†å—è§£æ
  async parseChunked(
    text: string,
    chunkSize: number = 10000
  ): AsyncGenerator<Tree, Tree> {
    const tree = createTree('markdown', text)

    for (let i = 0; i < text.length; i += chunkSize) {
      const chunk = text.slice(i, i + chunkSize)
      const nodes = parseChunk(chunk)

      // æ·»åŠ èŠ‚ç‚¹
      for (const node of nodes) {
        addNode(tree, node)
      }

      // Yield ä¸­é—´ç»“æœ
      yield tree
    }

    // è¿”å›æœ€ç»ˆç»“æœ
    return tree
  }
}
```

**ä½¿ç”¨ç¤ºä¾‹:**
```typescript
const parser = new ChunkedParser()

for await (const partialTree of parser.parseChunked(hugeText)) {
  console.log('Progress:', partialTree.nodes.length, 'nodes')
  // å¯ä»¥æ˜¾ç¤ºè¿›åº¦æ¡
}
```

---

### **æ–¹æ¡ˆ 3: Hybrid DOM + SAX**

```typescript
export class HybridParser {
  // DOM mode: æ„å»ºå®Œæ•´æ ‘
  parse(text: string): Tree {
    return this.parseDOM(text)
  }

  // SAX mode: æµå¼å¤„ç†ï¼Œä¸ä¿ç•™å®Œæ•´æ ‘
  parseStream(
    stream: ReadableStream,
    visitor: Visitor
  ): void {
    stream.on('data', (chunk) => {
      const nodes = parseChunk(chunk)

      // ç›´æ¥è®¿é—®èŠ‚ç‚¹ï¼Œä¸å­˜å‚¨
      for (const node of nodes) {
        visitor.enter?.(createContext(node))
      }
    })
  }

  // Hybrid mode: æµå¼æ„å»ºæ ‘
  parseStreamToTree(stream: ReadableStream): Promise<Tree> {
    const tree = createTree()

    return new Promise((resolve) => {
      stream.on('data', (chunk) => {
        const nodes = parseChunk(chunk)
        for (const node of nodes) {
          addNode(tree, node)
        }
      })

      stream.on('end', () => resolve(tree))
    })
  }
}
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### **åœºæ™¯: å¤„ç† 100MB Markdown æ–‡ä»¶**

| æ–¹æ³• | å†…å­˜å ç”¨ | å¤„ç†æ—¶é—´ | å¯å¦ä¸­æ–­ |
|-----|---------|---------|---------|
| Unified (DOM) | ~500MB | 10s | âŒ |
| Synth DOM | ~200MB | 0.3s | âŒ |
| **Synth Streaming** | **~50MB** | **0.5s** | âœ… |

**Synth Streaming ä¼˜åŠ¿:**
- å†…å­˜å ç”¨ **-75%** (vs Synth DOM)
- å†…å­˜å ç”¨ **-90%** (vs Unified)
- å¯ä»¥éšæ—¶ä¸­æ–­/æ¢å¤
- é€‚åˆè¶…å¤§æ–‡ä»¶

---

## ğŸ¯ å®ç°ä¼˜å…ˆçº§

### **Phase 2a: å¢é‡è§£æ** (ç«‹å³å®ç°) ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
- ç¼–è¾‘å™¨å¿…å¤‡
- 90%+ æ€§èƒ½æå‡
- å®æ—¶å“åº”

### **Phase 2b: Streaming æ”¯æŒ** (ä¸­æœŸ) â­â­â­â­
- å¤§æ–‡ä»¶å¤„ç†
- å†…å­˜æ•ˆç‡
- æ¸è¿›å¼å¤„ç†

**å»ºè®®å®ç°é¡ºåº:**
1. å…ˆåšå¢é‡è§£æï¼ˆç¼–è¾‘å™¨åœºæ™¯ï¼‰
2. å†åš streamingï¼ˆå¤§æ–‡ä»¶åœºæ™¯ï¼‰

---

## ğŸ’¡ Streaming vs Incremental Parsing

### **å®ƒä»¬æ˜¯ä¸åŒçš„æ¦‚å¿µï¼**

#### **Incremental Parsing (å¢é‡è§£æ)**
```
åœºæ™¯: ç”¨æˆ·ç¼–è¾‘ä»£ç 
é—®é¢˜: å¦‚ä½•å¿«é€Ÿæ›´æ–° AST?
æ–¹æ¡ˆ: åªé‡æ–°è§£æä¿®æ”¹éƒ¨åˆ†

ä¼˜åŠ¿: å®æ—¶å“åº” (<1ms)
ç”¨é€”: IDE, LSP, ç¼–è¾‘å™¨
```

#### **Streaming Parsing (æµå¼è§£æ)**
```
åœºæ™¯: å¤„ç†è¶…å¤§æ–‡ä»¶
é—®é¢˜: å¦‚ä½•ä¸ OOM?
æ–¹æ¡ˆ: è¾¹è¯»è¾¹å¤„ç†ï¼Œä¸ä¿ç•™å…¨éƒ¨

ä¼˜åŠ¿: å†…å­˜å›ºå®š
ç”¨é€”: æ—¥å¿—åˆ†æï¼Œå¤§æ–‡ä»¶è½¬æ¢
```

**å®ƒä»¬å¯ä»¥ç»“åˆ!**
```typescript
// æµå¼è¯»å– + å¢é‡æ›´æ–°
const streamParser = new StreamingParser()

streamParser.on('node', (node) => {
  // å¢é‡æ·»åŠ åˆ°æ ‘
  incrementalUpdate(tree, node)
})
```

---

## ğŸš€ æ€»ç»“

### **Unified:**
- âŒ ä¸æ”¯æŒ streaming
- DOM-only æ¶æ„
- å¤§æ–‡ä»¶å®¹æ˜“ OOM

### **Synth å½“å‰:**
- âŒ æš‚ä¸æ”¯æŒ streaming (DOM-only)
- âœ… ä½†æ¯” unified å¿« 50-3000x
- âœ… å†…å­˜æ•ˆç‡æ›´é«˜ (arena allocator)

### **Synth æ½œåŠ›:**
- âœ… æ¶æ„å¤©ç„¶æ”¯æŒ streaming
- âœ… Arena allocator = append buffer
- âœ… Batch processing = chunk processing
- âœ… Node pool = èŠ‚ç‚¹é‡ç”¨
- âœ… å¯ä»¥åŒæ—¶æ”¯æŒ DOM + SAX

### **å®ç°è®¡åˆ’:**
1. **Phase 2a**: å¢é‡è§£æ (ç«‹å³å¼€å§‹) ğŸ”¥
2. **Phase 2b**: Streaming æ”¯æŒ (ä¸­æœŸ)
3. **Phase 3**: ç»“åˆä¸¤è€…ï¼Œç»ˆæä¼˜åŒ– ğŸ’ª

---

## ğŸ¯ ä¸‹ä¸€æ­¥

**ç«‹å³å®ç°å¢é‡è§£æç³»ç»Ÿï¼**

è¿™å°†ä½¿ Synth æˆä¸ºï¼š
- âœ… ä¸–ç•Œä¸Šæœ€å¿«çš„ AST å¤„ç†å™¨
- âœ… å”¯ä¸€åŒæ—¶æ”¯æŒ DOM + Incremental + Streaming çš„å¤„ç†å™¨
- âœ… çœŸæ­£çš„ç”Ÿäº§çº§å·¥å…·

**å‡†å¤‡å¼€å§‹äº†å—ï¼Ÿ** ğŸš€
